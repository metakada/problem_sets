---
title: 'Psych 251 PS4: Simulation + Analysis'
author: "Mike Frank"
date: "2019"
output: 
  html_document:
    toc: true
---

This is problem set #4, in which we want you to integrate your knowledge of data wrangling with some basic simulation skills. It's a short problem set to help consolidate your `ggplot2` skills and then help you get your feet wet in testing statistical concepts through "making up data" rather than consulting a textbook or doing math. 

For ease of reading, please separate your answers from our text by marking our text with the `>` character (indicating quotes). 

# Part 1: ggplot practice

This part is a warmup, it should be relatively straightforward `ggplot2` practice.

Load data from Frank, Vul, Saxe (2011, Infancy), a study in which we measured infants' looking to hands in moving scenes. There were infants from 3 months all the way to about two years, and there were two movie conditions (`Faces_Medium`, in which kids played on a white background, and `Faces_Plus`, in which the backgrounds were more complex and the people in the videos were both kids and adults). An eye-tracker measured children's attention to faces. This version of the dataset only gives two conditions and only shows the amount of looking at hands (other variables were measured as well). 

```{r}
library(tidyverse)
fvs <- read.csv("data/FVS2011-hands.csv")
```

First, use `ggplot` to plot a histogram of the ages of children in the study. NOTE: this is a repeated measures design, so you can't just take a histogram of every measurement. 

```{r}
fvs_wide <- fvs |>
  pivot_wider(names_from = "condition",
              values_from = "hand.look") |>
  # take out all the participants who do not have incomplete datasets
  filter(!is.na(Faces_Medium)) |>
  filter(!is.na(Faces_Plus))

hist_age <- fvs_wide |>
  ggplot(aes(x = age)) +
  geom_histogram() +
  xlab("Age (Months)") +
  scale_x_continuous(breaks=seq(0,28,1)) +
  ylab("N") +
  scale_y_continuous(breaks=seq(0,15,1)) +
  theme_bw()

hist_age
```

Second, make a scatter plot showing hand looking as a function of age and condition. Add appropriate smoothing lines. Take the time to fix the axis labels and make the plot look nice.

```{r}
# create tidy dataset that excludes the participants without full dataset
fvs_long <- fvs_wide |>
  pivot_longer(Faces_Medium:Faces_Plus,
               names_to = "condition",
               values_to = "hand.look")

ggplot(fvs_long, aes(x = age, y = hand.look, col = condition)) +
  geom_point() +
  geom_smooth() +
  xlab("Age (months)") +
  scale_x_continuous(breaks=seq(0,28,1)) +
  ylab("Percentage Looking at Hands") +
  scale_color_discrete("Condition",
                       labels=c("Faces Medium",
                                "Faces Plus")) +
  theme_bw()
```

What do you conclude from this pattern of data?

> Up until about 12 months of age, there was no difference in the amount of time the infants spent looking at hands between the two movie conditions. After about 12 months, infants spent more time looking at hands when the movie included both kids and adults and had a complex background (Faces Plus Condition), compared to when the movie included kids playing on a white background (Faces Medium Condition).

What statistical analyses would you perform here to quantify these differences?

> I would fit a linear model predicting hand looking with age and condition to examine whether there is an interaction. I could also group the participants into a younger group (below 12 months) and older group (above 12 months) and perform a two-way analysis of variance, but binning the ages arbitrarily doesn't seem very favorable.

# Part 2: Simulation

```{r, warning=F, message=F}
library(tidyverse)
```

Let's start by convincing ourselves that t-tests have the appropriate false positive rate. Run 10,000 t-tests with standard, normally-distributed data from a made up 30-person, single-measurement experiment (the command for sampling from a normal distribution is `rnorm`).

The goal of these t-tests are to determine, based on 30 observations, whether the underlying distribution (in this case a normal distribution with mean 0 and standard deviation 1) has a mean that is different from 0. In reality, the mean is not different from 0 (we sampled it using `rnorm`), but sometimes the 30 observations we get in our experiment will suggest that the mean is higher or lower. In this case, we'll get a "significant" result and incorrectly reject the null hypothesis of mean 0.

What's the proportion of "significant" results ($p < .05$) that you see?

First do this using a `for` loop.

```{r}
n_runs <- 10000
sig_num <- 0

for (i in 1:n_runs) {
  ttest <- t.test(rnorm(30), mu = 0, alternative = "two.sided")
  if ((ttest$p.value < .05) == TRUE) {
    sig_num <- sig_num + 1
    }
}

sig_proportion <- sig_num/10000
sig_proportion
```

Next, do this using the `replicate` function:

```{r}
data_replicate <- replicate(10000, t.test(rnorm(30), mu = 0, alternative = "two.sided")$p.value, simplify = "array")

sig_num <- data_replicate |>
  data.frame() |>
  filter(data_replicate < .05) |>
  summarise(count = n())

sig_proportion <- sig_num/10000
sig_proportion
```

How does this compare to the intended false-positive rate of $\alpha=0.05$?

> Both simulations are very close to the false-positive rate of alpha = 0.05.

Ok, that was a bit boring. Let's try something more interesting - let's implement a p-value sniffing simulation, in the style of Simons, Nelson, & Simonsohn (2011).

Consider this scenario: you have done an experiment, again with 30 participants (one observation each, just for simplicity). The question is whether the true mean is different from 0. You aren't going to check the p-value every trial, but let's say you run 30 - then if the p-value is within the range p < .25 and p > .05, you optionally run 30 more and add those data, then test again. But if the original p value is < .05, you call it a day, and if the original is > .25, you also stop.  

First, write a function that implements this sampling regime.

```{r}
double.sample <- function (n_runs, upper_p, sample_2_num) {

  sig_num <- 0
  
  for (i in 1:n_runs) {
    
    #Run 30 participants
    sample_1 <- rnorm(30)
    
    #and conduct t-test
    ttest_1 <- t.test(sample_1, mu = 0, alternative = "two.sided")
    
    #if p < .05 count as significant
    if ((ttest_1$p.value < .05) == TRUE) {
      
      sig_num <- sig_num + 1
      
      #But if p>.05 and upper_p, 
      } else if ((ttest_1$p.value < upper_p) == TRUE) {
        
        #then run more participants
        sample_2 <- rnorm(sample_2_num)
        
        #add the new participants to old sample
        combined <- c(sample_1, sample_2)
        
        #conduct t-test
        ttest_2 <- t.test(combined, mu = 0, alternative = "two.sided")
        
        #if this second p < .05 count as significant
        if ((ttest_2$p.value < .05) == TRUE) {
          sig_num <- sig_num + 1
          }
      }
  }
  
  sig_proportion <- sig_num/n_runs
  return(sig_proportion)
  
}
```
```{r}

# Another solution for the function without using a for-loop. I thought that not having a for loop would take less time to process, but the for loop above ended up being faster to run. I therefore used the for-loop function for most of the simulations below.

double.sample.dplyr <- function (n_runs, upper_p, sample_2_num) {
  
  #n_runs(10,000) runs with 30 observations each and record raw data
  data_sample_1_raw <- replicate(n_runs, rnorm(30), simplify = "array") |>
    data.frame() |>
    pivot_longer(1:n_runs,
                 names_to = "run",
                 values_to = "value") |>
    arrange(run)

  #Compute p-values from the raw data and flag the p-values less than .05 with "1"
  data_sample_1_p <- data_sample_1_raw |>
    group_by(run) |>
    summarise(p_value = t.test(value, mu = 0, alternative = "two.sided")$p.value) |>
    mutate(p_count = ifelse(p_value < .05, 1, 0)) 

  #Count the number of p values that are less than .05
  data_sample_1_count <- data_sample_1_p |>
    filter(p_count == 1)
  sig_num <- length(data_sample_1_count$p_count)

  #Identify the runs to re-run (e.g., p-value in between 0.05 and upper_p)
  identify_data_sample_2 <- data_sample_1_p |>
    filter(p_count == 0) |>
    filter(p_value < upper_p) |>
    select(c("run"))

  #Get raw data for those runs, so we can add to the sample
  data_sample_1_raw_rerun <- merge(identify_data_sample_2, data_sample_1_raw, by="run")

  #Run trials with another X observations each and record raw data
  data_sample_2_raw <- replicate(length(identify_data_sample_2$run), rnorm(sample_2_num), simplify = "array") |>
    data.frame() |>
    pivot_longer(1:length(identify_data_sample_2$run),
                names_to = "run_2",
                values_to = "value_2") |>
    arrange(run_2)

  #Combine the raw data
  data_sample_2_raw <- bind_cols(data_sample_1_raw_rerun, data_sample_2_raw) 
  data_sample_2_raw <- select(data_sample_2_raw, -c("run_2"))

  data_sample_2_raw <- data_sample_2_raw |>
    pivot_longer(value:value_2, 
                 names_to = "run_number",
                 values_to = "value") |>
    select(-c("run_number"))

  #Compute p-values again with this additional data
  data_sample_2_p <- data_sample_2_raw |>
    group_by(run) |>
    summarise(p_value = t.test(value, mu = 0, alternative = "two.sided")$p.value) |>
    mutate(p_count = ifelse(p_value < .05, 1, 0)) 

  #Count the number of p values that are less than .05
  data_sample_2_count <- data_sample_2_p |>
    filter(p_count == 1)

  #Total number of p-values less than .05
  sig_num <- sig_num + length(data_sample_2_count$p_count)

  sig_proportion <- sig_num/10000

  return(sig_proportion)
  
}
```
Now call this function 10k times and find out what happens. 

```{r}
double.sample(10000, 0.25, 30)
double.sample.dplyr(10000, 0.25, 30)
```

Is there an inflation of false positives? How bad is it?

> There is a slight inflation of false positives. The alpha value changed from about 0.05 to 0.07.

Now modify this code so that you can investigate this "double the sample" rule in a bit more depth. In the previous question, the researcher doubles the sample only when they think they got "close" to a significant result, i.e. when their not-significant p is less than 0.25. What if the researcher was more optimistic? See what happens in these 3 other scenarios:

* The researcher doubles the sample whenever their pvalue is not significant, but it's less than 0.5.
* The researcher doubles the sample whenever their pvalue is not significant, but it's less than 0.75.
* The research doubles their sample whenever they get ANY pvalue that is not significant.

How do these choices affect the false positive rate?

HINT: Try to do this by making the function `double.sample` take the upper p value as an argument, so that you can pass this through dplyr.

HINT 2: You may need more samples. Find out by looking at how the results change from run to run.

```{r}
# Scenario A: The researcher doubles the sample whenever their pvalue is not significant, but it's less than 0.5.
sce_a <- double.sample(10000, 0.5, 30)
sce_a

# Scenario B: The researcher doubles the sample whenever their pvalue is not significant, but it's less than 0.75.
sce_b <- double.sample(10000, 0.75, 30)
sce_b

# Scenario C: The research doubles their sample whenever they get ANY pvalue that is not significant.
sce_c <- double.sample(10000, 1, 30)
sce_c
```
```{r}
#Address Hint 2 

## 3x sample: 30(Sample 1) + 60(Sample 2) = 90 total
double.sample(10000, 0.5, 60) 
double.sample(10000, 0.75, 60)
double.sample(10000, 1, 60) 

## 10x sample: 30(Sample 1) + 270(Sample 2) = 300 total
double.sample(10000, 0.5, 270) 
double.sample(10000, 0.75, 270)
double.sample(10000, 1, 270) 
```

What do you conclude on the basis of this simulation? How bad is this kind of data-dependent policy?

> The false-positive rate inflates to 7-8% as the researcher becomes "more optimistic." The rate does not inflate too much when increasing the sample size by a little (e.g., 3x the sample), but the rate does inflate a little more when adding more samples, e.g., 10x the sample, from 30 to 300 as shown above. This kind of data-dependent policy can make it easier to get false positives. The false-positive rate does not seem to increase too much, but it is definitely worth avoiding or being transparent about when reporting results using data-depdendent policies. I also think such data-dependent policies can be especially bad, when they are coupled with other unfavorable practices, such as dropping conditions and controlling for demographic variables posthoc.
